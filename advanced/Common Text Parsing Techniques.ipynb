{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Text Parsing Techniques\n",
    "\n",
    "Working with text data is annoying and hard because text is generated by people, and people are awful. We use all kinds of dumb words and symbols and complicated stuff that makes everything very confusing. Text parsing is the process of trying to make text data more like other kinds of data - numbers, categories etc. Generally, this falls into two distinct but related processes - text \"cleaning\", which is about removing extraneous or confusing details to standardise the text and reduce \"noise\", and text \"representation\", which is about turning text data into a numeric representation to make it accessible to both simple analysis (counting, scoring, etc.) and complex algorithms (similarity scores, categorisation, prediction etc.)\n",
    "\n",
    "In this tutorial I'll go through some common techniques for cleaning up text. These are often used as the precursor to more complex analysis of text data. As usual, I'll try to show you what I think is the _one, best way_ to do this, as well as give a bit of insight into how this works behind the scenes. This is a bit trickier for this tutorial though, because the right way to do this is very dependant on the eventual goal of your analysis, and a lot of these techniques use very complex processes to achieve something that looks quite simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import unicodedata\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nltk` module is one of the most common tools used for working with text data. Thing is, it's _huge_. It includes huge amounts of text \"corpuses\", and other data sets. So when you first install it it comes with only the bare minimum of these. The extra stuff you have to download as you need it. Fortunately it provides a convenient method for doing that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\simon.carryer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\simon.carryer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\simon.carryer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\simon.carryer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we're going to use is several years' worth of entries in Wellington, New Zealand's \"Burger Wellington\" competition, where restaurants create and sell a fancy burger for the event. The dataset contains the burger name, the restaurant name, and a description of the burger, as well as some other details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/SimonCarryer/burgerwellington/master/burgers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Burger Name', 'Burger Description', 'Restaurant', 'Price', 'Year',\n",
       "       'Beef', 'Chicken', 'Duck', 'Lamb', 'Pork', 'Seafood', 'Sweet',\n",
       "       'Vegetarian', 'Venison', 'Not Your Usual', 'Finalist', 'Winner',\n",
       "       'Restaurant_cleaned'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String cleaning\n",
    "\n",
    "String cleaning mean removing \"noise\" from text. Generally, it's all about making the text more \"regular\" - reducing the number of different characters and the variety of representations of the same information. The challenge is that all \"noise\" is also information. Consider - removing capital letters removes the distinction between \"Grey\" the name, and \"grey\" the colour. Removing punctuation removes the distinction between \"its\" and \"it's\", and removes the breaks between sentences.\n",
    "\n",
    "In the case of this burger data, that's not a problem, but for other uses, it might be very important to keep some of that information. You should fit your string cleaning approach to the purpose of your analysis.\n",
    "\n",
    "String cleaning can also be very time-consuming. If you have millions of rows of text, or very long documents, you might look for more efficient approaches than what I use here.\n",
    "\n",
    "Here are a bunch of functions that take a single `string` input, and clean that `string` in one specific way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(text):\n",
    "    \"Convert BIG BOYS to wee chaps\"\n",
    "    return text.lower()\n",
    "\n",
    "def remove_text_in_parentheses(text):\n",
    "    \"\"\"Removes any text (enclosed in parentheses)\"\"\"\n",
    "    return re.sub(\"\\(.*?\\)\", \"\", text)\n",
    "\n",
    "def convert_special_characters(text):\n",
    "    \"Replace fancy characters with the nearest ascii equivalent\"\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode() # https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize\n",
    "\n",
    "def normalise_whitespace(text):\n",
    "    \"Tidy up multiple spaces, newlines, and leading/trailing spaces.\"\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def hyphens_to_spaces(text):\n",
    "    \"\"\"Replaces hyphens and slashes with spaces.\"\"\"\n",
    "    return text.replace(\"-\", \" \").replace(\"/\", \" \")\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"Removes all punctuation (as defined by string.punctuation)\"\"\"\n",
    "    return text.translate(str.maketrans('', '', string.punctuation)) # https://python-reference.readthedocs.io/en/latest/docs/str/translate.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on what you need, you might only want some of the above functions, or you might want to write new functions or your own versions of the above. For example, you might want to remove only some kinds of punctuation, or you might want to preserve newline characters.\n",
    "\n",
    "When you know exactly which functions you need (and importantly the order in which to apply them), it's handy to have a single function that applies them all in the correct order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, functions=[remove_text_in_parentheses, hyphens_to_spaces, convert_special_characters, remove_punctuation, lowercase, normalise_whitespace]):\n",
    "    for function in functions:\n",
    "        text = function(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well hello there how are you'"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"well â€” hello therÃ©. ðŸ˜‹  how are you? (we don't want this bit) \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `pandas` you can apply that function to a `Series` using the `Series.apply` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           1815 cafe bar\n",
       "1        annam restaurant\n",
       "2                  apache\n",
       "3    artisan dining house\n",
       "4           aston norwood\n",
       "Name: Restaurant, dtype: object"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Restaurant\"].apply(clean_text).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Excercises:__\n",
    "* Apply the `clean_text` function to the `Burger Name` column.\n",
    "* How many unique restaurant names are there in the dataset before cleaning, compared to after?\n",
    "* Run the `clean_text` function without removing punctuation.\n",
    "* __HARD MODE__: Make a function that replaces numbers with a dummy value, so \"1815 cafe bar\" becomes \"XXXX cafe bar\", and include it in the `clean_text` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "\n",
    "\"Bag of Words\" is a form of text \"representation\" or \"embedding\". In other words, it's a way of turning your text data into numeric data. Bag of words is one of the most basic ways of doing this, but it's also extremely useful. Essentially it means learning a \"vocabulary\" of all the words in your text, and then making a dataset that's got one column for every word, one row for every document, and a count of how many times that word appears in the document.\n",
    "\n",
    "For example, \"the cat sat on the mat\" would be represented as:\n",
    "\n",
    "|the|cat|sat|on|mat|\n",
    "|-|-|-|-|-|\n",
    "|2|1|1|1|1|\n",
    "\n",
    "If we had another document in our corpus, say \"the cat ate the rat\", our bag of words representation would look like this:\n",
    "\n",
    "|the|cat|sat|on|mat|ate|rat|\n",
    "|-|-|-|-|-|-|-|\n",
    "|2|1|1|1|1|0|0|\n",
    "|2|1|0|0|0|1|1|\n",
    "\n",
    "A few this worth noting here: \n",
    "* Bag of words does not preserve the order of words. \"The cat sat on the mat\" and \"the mat sat on the cat\" would be represented the same way. \n",
    "* Bag of words quickly grows to have a very wide dataset, with a lot of columns. String cleaning can help reduce this (so \"It's\", \"it's\", \"its\" and \"Its\" all get one column, rather than four columns, for example), but you will still end up with very wide tables.\n",
    "* You also get a very sparse dataset, where most rows contain mostly zeros. Most words don't show up in most documents.\n",
    "\n",
    "To create a bag of words embedding, we use the `CountVectorizer` class from the `sklearn` module.\n",
    "\n",
    "This class does two main things:\n",
    "\n",
    "* `fit` takes a column of text (a set of \"documents\") and learns a vocabulary from them (a list of all the words in the documents).\n",
    "* `transform` takes a set of documents and converts them into a bag-of-words embedding, with one row for each document and one column for each word.\n",
    "* It also has a `fit_transform` method which conveniently lets you do both of the above things in one step.\n",
    "\n",
    "By default the `CountVectorizer` will lower-case the text, and there's a `strip_accents` argument you can pass to make it normalise unusual characters too. For more fine-grained control you can pass your own text-cleaning function to the `preprocesser` keyword. We can use the `clean_text` function we wrote.\n",
    "\n",
    "I strongly suggest checking out the documentation for the `CountVectorizer` class, as it has a lot of useful functions.\n",
    "\n",
    "__NOTE:__ We _could_ create new columns in our `DataFrame` holding the cleaned version of each of the text columns, and do all our analysis on those. That would be a good option if we expected to do a lot of exploratory analysis on those columns, and it would be faster to clean the text only once. But! Another common use of the `CountVectorizer` is to use it as the first step in building some kind of text classification model. In those cases it's important that the training data and the test data are cleaned exactly the same way, and for that purpose having everything contained within the `CountVectorizer` object is very convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(preprocessor=clean_text)\n",
    "\n",
    "bag_of_words = vec.fit_transform(df[\"Restaurant\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<788x526 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2054 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's this? the bag of words that the `Countvectorizer` has returned isn't a `DataFrame`, it's a `sparse_matrix`. What the heck is that? Remember how bag of words returns a very wide table, where most of the values are zero? To save on space and to make a big dataset like this easier to work with, the `Countvectorizer` returns a `sparse_matrix` format, which more efficiently represents this data. Instead of storing a value for every row and column, it only stores information about non-zero values, in the form of a tuple formatted like \"(<count>,<row_index>,<col_index>)\". For example `(1, 0, 2)` means \"There was 1 instance in the first document of the third word in the vocabulary\".\n",
    "    \n",
    "If you're working with large datasets, you might need to keep your data in `sparse_matrix` format. But for smaller datasets, or where you really need to see what's going on better, you can turn this representation back into a `DataFrame` pretty easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_of_words = pd.DataFrame(bag_of_words.todense(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1154</th>\n",
       "      <th>169</th>\n",
       "      <th>1815</th>\n",
       "      <th>20</th>\n",
       "      <th>44</th>\n",
       "      <th>88</th>\n",
       "      <th>absolute</th>\n",
       "      <th>ale</th>\n",
       "      <th>ales</th>\n",
       "      <th>alicetown</th>\n",
       "      <th>...</th>\n",
       "      <th>willis</th>\n",
       "      <th>wilson</th>\n",
       "      <th>wine</th>\n",
       "      <th>wood</th>\n",
       "      <th>woodfire</th>\n",
       "      <th>woods</th>\n",
       "      <th>woodshed</th>\n",
       "      <th>yard</th>\n",
       "      <th>zake</th>\n",
       "      <th>zibibbo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 526 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1154  169  1815  20  44  88  absolute  ale  ales  alicetown  ...  willis  \\\n",
       "0     0    0     1   0   0   0         0    0     0          0  ...       0   \n",
       "1     0    0     0   0   0   0         0    0     0          0  ...       0   \n",
       "2     0    0     0   0   0   0         0    0     0          0  ...       0   \n",
       "3     0    0     0   0   0   0         0    0     0          0  ...       0   \n",
       "4     0    0     0   0   0   0         0    0     0          0  ...       0   \n",
       "\n",
       "   wilson  wine  wood  woodfire  woods  woodshed  yard  zake  zibibbo  \n",
       "0       0     0     0         0      0         0     0     0        0  \n",
       "1       0     0     0         0      0         0     0     0        0  \n",
       "2       0     0     0         0      0         0     0     0        0  \n",
       "3       0     0     0         0      0         0     0     0        0  \n",
       "4       0     0     0         0      0         0     0     0        0  \n",
       "\n",
       "[5 rows x 526 columns]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_of_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the structure of the data much more easily. As expected, most of the values are zero.\n",
    "\n",
    "The instance of the `CountVectorizer` class that we created, `vec`, has been `fit` with the cleaned restaurant names. We can see the vocabulary it learnt by calling its `get_feature_names` method. If you call `transform` on a new set of documents, it will use the vocabulary it already learnt (in other words, it will return the same columns, even if there are new words in the new set of documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't know anything about the frequencies of those words though. To see that, we have to look at the bag of words `DataFrame`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the           148\n",
       "bar           130\n",
       "cafe           80\n",
       "restaurant     75\n",
       "and            37\n",
       "             ... \n",
       "jones           1\n",
       "kk              1\n",
       "lab             1\n",
       "lady            1\n",
       "1154            1\n",
       "Length: 526, dtype: int64"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_of_words.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this `DataFrame`, you can now do anything you'd do with any other `DataFrame`. For example, grouping, taking sums, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1154</th>\n",
       "      <th>169</th>\n",
       "      <th>1815</th>\n",
       "      <th>20</th>\n",
       "      <th>44</th>\n",
       "      <th>88</th>\n",
       "      <th>absolute</th>\n",
       "      <th>ale</th>\n",
       "      <th>ales</th>\n",
       "      <th>alicetown</th>\n",
       "      <th>...</th>\n",
       "      <th>willis</th>\n",
       "      <th>wilson</th>\n",
       "      <th>wine</th>\n",
       "      <th>wood</th>\n",
       "      <th>woodfire</th>\n",
       "      <th>woods</th>\n",
       "      <th>woodshed</th>\n",
       "      <th>yard</th>\n",
       "      <th>zake</th>\n",
       "      <th>zibibbo</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 526 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1154  169  1815  20  44  88  absolute  ale  ales  alicetown  ...  \\\n",
       "Year                                                               ...   \n",
       "2014     0    0     0   0   1   0         0    0     0          0  ...   \n",
       "2015     0    0     0   0   1   1         1    0     0          0  ...   \n",
       "2016     0    0     0   0   0   0         0    0     0          0  ...   \n",
       "2017     0    0     1   0   0   0         0    0     0          0  ...   \n",
       "2018     0    0     1   0   0   0         0    0     1          0  ...   \n",
       "2019     1    1     1   1   0   0         0    1     1          1  ...   \n",
       "\n",
       "      willis  wilson  wine  wood  woodfire  woods  woodshed  yard  zake  \\\n",
       "Year                                                                      \n",
       "2014       0       0     0     0         0      0         0     0     0   \n",
       "2015       0       0     0     0         0      0         0     0     0   \n",
       "2016       0       0     0     0         0      1         1     0     0   \n",
       "2017       0       0     0     0         1      1         0     1     0   \n",
       "2018       1       1     0     1         1      1         0     1     0   \n",
       "2019       1       2     1     1         0      1         0     1     1   \n",
       "\n",
       "      zibibbo  \n",
       "Year           \n",
       "2014        0  \n",
       "2015        1  \n",
       "2016        1  \n",
       "2017        1  \n",
       "2018        1  \n",
       "2019        0  \n",
       "\n",
       "[6 rows x 526 columns]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_of_words.groupby(df[\"Year\"]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Excercises:__\n",
    "* Check out the documentation for the `CountVectorizer` class. Are there any other useful arguments you can pass it?\n",
    "* Create a bag of words embedding of the `Burger Description` column. Don't forget to clean the strings first.\n",
    "* What's the most common word in the burger names for each year?\n",
    "* __HARD MODE__: Create a bag of words embedding for all the burgers, but using only words that appear in burgers from the first year of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenising\n",
    "\n",
    "Remember how when we made that bag of words embedding, it made one column for every word in the corpus? How did it know what a \"word\" is? It split the documents on \"white space\". This process of splitting documents up into smaller chunks is called \"tokenising\", and there are a lot of different ways to do it. Splitting on white-space is the simplest way to do it, but there are other ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Embeddings\n",
    "\n",
    "Rather than splitting text up into its constituent words, you can split it up into characters. This substantially reduces the number of tokens (if you've cleaned you're text thoroughly, you'll have at most the 26 letters of the English alphabet, plus numbers maybe). On the other hand, since it doesn't preserve order, you lose a _lot_ of information.\n",
    "\n",
    "A cool think about the `CountVectorizer` is that it comes pre-built with a few different ways of doing tokenization. You control that by passing a different magic word to the `analyzer` keyword argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>a</th>\n",
       "      <th>...</th>\n",
       "      <th>q</th>\n",
       "      <th>r</th>\n",
       "      <th>s</th>\n",
       "      <th>t</th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>w</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1  2  3  4  5  7  8  a  ...  q  r  s  t  u  v  w  x  y  z\n",
       "0  2  0  0  0  0  0  0  0  0  0  ...  0  0  2  1  2  0  0  0  0  0\n",
       "1  2  0  0  0  0  0  0  0  0  0  ...  0  0  1  2  1  0  0  0  1  0\n",
       "2  2  0  0  0  0  0  0  0  0  0  ...  0  0  0  0  2  0  0  0  0  0\n",
       "3  2  0  0  0  0  0  0  0  0  1  ...  0  0  1  2  0  0  0  0  0  0\n",
       "4  2  0  0  0  0  0  0  0  0  2  ...  0  3  1  1  1  0  0  0  0  0\n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer(analyzer=\"char\", preprocessor=clean_text)\n",
    "bag_of_words = vec.fit_transform(df[\"Burger Name\"])\n",
    "df_of_words = pd.DataFrame(bag_of_words.todense(), columns=vec.get_feature_names())\n",
    "df_of_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams\n",
    "\n",
    "That character embedding above doesn't seem very useful, but there's a way to make it much more useful, which is to employ \"n-grams\". \"N-Grams\" means analysing overlapping _sets_ of tokens, rather than single tokens. A set of two tokens is called a \"2-gram\", a set of three is a \"3-gram\" and so on. For example, if we were breaking the word \"burger\" into character 2-grams, we'd get the following 2-grams: \"bu\", \"ur\", \"rg\", \"ge\" and \"er\".\n",
    "\n",
    "This is neat because it retains more information about the order of the characters, while still keeping the number of distinct tokens fairly low.\n",
    "\n",
    "Again, the `CountVectorizer` has a built-in method for creating n-grams, controlled by the `ngram_range` keyword argument. It expects a tuple in the format `(<minimum_gram_length>,<maximum_gram_length>)`. So, for example, passing `(1, 3)` means that you want to break your documents into all their constituent 1-grams, 2-grams, _and_ 3-grams. Passing `(3, 3)` means getting just the 3-grams.\n",
    "\n",
    "Don't forget that spaces are still characters, so by default you'll get character n-grams with spaces in them from the start and end of words, like \" a\" and \"t \". Usually that's a good thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>7</th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>...</th>\n",
       "      <th>z</th>\n",
       "      <th>za</th>\n",
       "      <th>ze</th>\n",
       "      <th>zi</th>\n",
       "      <th>zl</th>\n",
       "      <th>zo</th>\n",
       "      <th>zs</th>\n",
       "      <th>zu</th>\n",
       "      <th>zy</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 533 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        1   2   3   4   5   7   a   b   c  ...  z   za  ze  zi  zl  zo  zs  \\\n",
       "0  22   0   0   0   0   0   0   3   2   3  ...   0   1   1   0   0   0   0   \n",
       "1  17   0   0   0   0   0   0   1   0   3  ...   0   0   0   0   0   0   0   \n",
       "2  27   0   0   0   0   0   0   3   1   7  ...   0   0   0   0   0   0   0   \n",
       "3  21   0   0   0   0   0   0   4   3   2  ...   0   0   0   0   0   0   0   \n",
       "4  19   0   0   0   0   0   0   2   3   3  ...   0   0   0   0   0   0   0   \n",
       "\n",
       "   zu  zy  zz  \n",
       "0   0   0   0  \n",
       "1   0   0   0  \n",
       "2   0   0   0  \n",
       "3   0   0   0  \n",
       "4   0   0   0  \n",
       "\n",
       "[5 rows x 533 columns]"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer(analyzer=\"char\", ngram_range=(1, 2), preprocessor=clean_text)\n",
    "bag_of_words = vec.fit_transform(df[\"Burger Description\"])\n",
    "df_of_words = pd.DataFrame(bag_of_words.todense(), columns=vec.get_feature_names())\n",
    "df_of_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word tokens can also be turned into n-grams, though be aware that this will often substantially increase the size of your vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>12 hour</th>\n",
       "      <th>12 pound</th>\n",
       "      <th>14 lb</th>\n",
       "      <th>15 hour</th>\n",
       "      <th>18 hour</th>\n",
       "      <th>200 gram</th>\n",
       "      <th>200g beef</th>\n",
       "      <th>200gm medium</th>\n",
       "      <th>4s chicken</th>\n",
       "      <th>72 dark</th>\n",
       "      <th>...</th>\n",
       "      <th>zeus smoky</th>\n",
       "      <th>zeus southland</th>\n",
       "      <th>zeus tzatziki</th>\n",
       "      <th>zeus yoghurt</th>\n",
       "      <th>zucchini and</th>\n",
       "      <th>zucchini citrus</th>\n",
       "      <th>zucchini cucumber</th>\n",
       "      <th>zucchini fries</th>\n",
       "      <th>zucchini pickle</th>\n",
       "      <th>zucchini pickles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 7939 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   12 hour  12 pound  14 lb  15 hour  18 hour  200 gram  200g beef  \\\n",
       "0        0         0      0        0        0         0          0   \n",
       "1        0         0      0        0        0         0          0   \n",
       "2        0         0      0        0        0         0          0   \n",
       "3        0         0      0        0        0         0          0   \n",
       "4        0         0      0        0        0         0          0   \n",
       "\n",
       "   200gm medium  4s chicken  72 dark  ...  zeus smoky  zeus southland  \\\n",
       "0             0           0        0  ...           0               0   \n",
       "1             0           0        0  ...           0               0   \n",
       "2             0           0        0  ...           0               0   \n",
       "3             0           0        0  ...           0               0   \n",
       "4             0           0        0  ...           0               0   \n",
       "\n",
       "   zeus tzatziki  zeus yoghurt  zucchini and  zucchini citrus  \\\n",
       "0              0             0             0                0   \n",
       "1              0             0             0                0   \n",
       "2              0             0             0                0   \n",
       "3              0             0             0                0   \n",
       "4              0             0             0                0   \n",
       "\n",
       "   zucchini cucumber  zucchini fries  zucchini pickle  zucchini pickles  \n",
       "0                  0               0                0                 0  \n",
       "1                  0               0                0                 0  \n",
       "2                  0               0                0                 0  \n",
       "3                  0               0                0                 0  \n",
       "4                  0               0                0                 0  \n",
       "\n",
       "[5 rows x 7939 columns]"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer(analyzer=\"word\", ngram_range=(2, 2), preprocessor=clean_text)\n",
    "bag_of_words = vec.fit_transform(df[\"Burger Description\"])\n",
    "df_of_words = pd.DataFrame(bag_of_words.todense(), columns=vec.get_feature_names())\n",
    "df_of_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Excercises:__\n",
    "* Make a `DataFrame` of the character 2-grams and 3-grams in all the burger names.\n",
    "* What's the most commonly-ocurring word 2-gram in burger descriptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop-words\n",
    "\n",
    "Remember back when we looked at the most-common words in the burger descriptions, and the top few words were things like \"with\" and \"and\"? That's not very exciting, right? Usually we don't care about these kind of \"filler\" words. A common technique is to just remove these - when you do that, the list of words you remove is called you \"stop-words\".\n",
    "\n",
    "Conveniently, the `CountVectorizer` comes with a pre-configured set of English stop words, which you can exclude automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "burger     196\n",
       "chicken     22\n",
       "beef        19\n",
       "lamb        16\n",
       "sweet       12\n",
       "big         11\n",
       "deer        11\n",
       "la          10\n",
       "pig         10\n",
       "piggy       10\n",
       "dtype: int64"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer(stop_words=\"english\", preprocessor=clean_text)\n",
    "bag_of_words = vec.fit_transform(df[\"Burger Name\"])\n",
    "df_of_words = pd.DataFrame(bag_of_words.todense(), columns=vec.get_feature_names())\n",
    "df_of_words.sum().sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass `stop_words` your own list of words. If you need access to your own list of stop-words, `nltk` has one which you can use. Here's some good advice on doing that, from the `sklearn` library: https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any cleaning you do on your text you should also do on your stop words, to ensure that they match correctly (things like removing punctuation turning \"you're\" to \"youre\" and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'youre']"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_stops = [clean_text(word) for word in stopwords.words('english')]\n",
    "custom_stops[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Tokenising\n",
    "\n",
    "Maybe the way you want to split up your text isn't by word _or_ by character. Maybe you've got your own way you want to do that which uses your own particular logic. We can support that too!\n",
    "\n",
    "The `CountVectorizer` takes a `tokenizer` keyword argument, which should be a method that takes a string and returns a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>20</th>\n",
       "      <th>3</th>\n",
       "      <th>44</th>\n",
       "      <th>4t</th>\n",
       "      <th>a</th>\n",
       "      <th>ag</th>\n",
       "      <th>al</th>\n",
       "      <th>an</th>\n",
       "      <th>ap</th>\n",
       "      <th>ar</th>\n",
       "      <th>...</th>\n",
       "      <th>yu</th>\n",
       "      <th>z</th>\n",
       "      <th>z b</th>\n",
       "      <th>za</th>\n",
       "      <th>zen</th>\n",
       "      <th>zer</th>\n",
       "      <th>zeu</th>\n",
       "      <th>zil</th>\n",
       "      <th>zo</th>\n",
       "      <th>zo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1719 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    20   3   44   4t   a    ag   al   an   ap   ar  ...  yu   z  z b  za   \\\n",
       "0    0   0    0    0    0    0    0    0    0    0  ...    0  0    0    0   \n",
       "1    0   0    0    0    0    0    0    0    0    0  ...    0  0    0    0   \n",
       "2    0   0    0    0    0    0    0    0    0    0  ...    0  0    0    0   \n",
       "3    0   0    0    0    0    0    0    0    0    0  ...    0  0    0    0   \n",
       "4    0   0    0    0    0    0    0    0    0    0  ...    0  0    0    0   \n",
       "\n",
       "   zen  zer  zeu  zil  zo  zo   \n",
       "0    0    0    0    0   0    0  \n",
       "1    0    0    0    0   0    0  \n",
       "2    0    0    0    0   0    0  \n",
       "3    0    0    0    0   0    0  \n",
       "4    0    0    0    0   0    0  \n",
       "\n",
       "[5 rows x 1719 columns]"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenise(text):\n",
    "    return [text[i:i+3] for i in range(0, len(text), 3)] # Split text into non-overlapping 3-character tokens for some reason\n",
    "\n",
    "vec = CountVectorizer(tokenizer=tokenise, preprocessor=clean_text)\n",
    "bag_of_words = vec.fit_transform(df[\"Burger Name\"])\n",
    "df_of_words = pd.DataFrame(bag_of_words.todense(), columns=vec.get_feature_names())\n",
    "df_of_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatising\n",
    "\n",
    "Stemming and Lemmatising are two complex subjects that I'll only touch on briefly here. They're both ways of reducing the complexity of text by attempting to trim words down to a \"root\" - removing all the inflection that languages add - \"Jump\", \"jumping\", \"jumps\", \"jumped\" all become \"jump\" for example.\n",
    "\n",
    "\"Stemming\" is a kind of brute-force approach that looks for common inflection endings (\"-e\", \"-ing\", \"-ed\") and snips them off. \n",
    "\n",
    "\"Lemmatising\" is a more complex (and therefore slower) approach that more accurately reduces words to their roots (or \"lemmas\"), by using information about what part of speech the word represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'heres an example of a sentence that get some benefits from stemming lemmatising or it might not'"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text = clean_text(\"Here's an example of a sentence that get some benefits from stemming/lemmatising - or it might not!\")\n",
    "example_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nltk` provides a few stemmer classes, of which `PorterStemmer` is probably the most widely-used. You can apply stemming as part of your tokenising step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenise(text):\n",
    "    return [porter.stem(w) for w in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['here',\n",
       " 'an',\n",
       " 'exampl',\n",
       " 'of',\n",
       " 'a',\n",
       " 'sentenc',\n",
       " 'that',\n",
       " 'get',\n",
       " 'some',\n",
       " 'benefit',\n",
       " 'from',\n",
       " 'stem',\n",
       " 'lemmatis',\n",
       " 'or',\n",
       " 'it',\n",
       " 'might',\n",
       " 'not']"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenise(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the stemmer has been a bit over-zealous, snipping the \"e\" off the end of \"example\" and \"sentence\", but its turned \"here's\" into \"here\" and \"stemming\" into \"stem\", which is good.\n",
    "\n",
    "We can use stemming in our `CountVectorizer` by passing the new `tokenise` method to its `tokenizer` keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(tokenizer=tokenise, preprocessor=clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>12</th>\n",
       "      <th>15</th>\n",
       "      <th>18</th>\n",
       "      <th>2</th>\n",
       "      <th>200</th>\n",
       "      <th>200g</th>\n",
       "      <th>200gm</th>\n",
       "      <th>25</th>\n",
       "      <th>...</th>\n",
       "      <th>yum</th>\n",
       "      <th>yuzu</th>\n",
       "      <th>zaatar</th>\n",
       "      <th>zaida</th>\n",
       "      <th>zani</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zelati</th>\n",
       "      <th>zeppelin</th>\n",
       "      <th>zeu</th>\n",
       "      <th>zucchini</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1659 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  10  12  15  18  2  200  200g  200gm  25  ...  yum  yuzu  zaatar  zaida  \\\n",
       "0  0   0   0   0   0  0    0     0      0   0  ...    0     0       0      0   \n",
       "1  0   0   0   0   0  0    0     0      0   0  ...    0     0       0      0   \n",
       "2  0   0   0   0   0  0    0     0      0   0  ...    0     0       0      0   \n",
       "3  0   0   0   0   0  0    0     0      0   0  ...    0     0       0      0   \n",
       "4  0   0   0   0   0  0    0     0      0   0  ...    0     0       0      0   \n",
       "\n",
       "   zani  zealand  zelati  zeppelin  zeu  zucchini  \n",
       "0     1        0       0         0    1         0  \n",
       "1     0        0       0         0    0         0  \n",
       "2     0        0       0         0    0         0  \n",
       "3     0        0       0         0    0         0  \n",
       "4     0        0       0         0    0         0  \n",
       "\n",
       "[5 rows x 1659 columns]"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words = vec.fit_transform(df[\"Burger Description\"])\n",
    "pd.DataFrame(bag_of_words.todense(), columns=vec.get_feature_names()).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatising is a bit more complicated, because we need to get the \"part of speech\" for each word that we want to lemmatise and, frustratingly, the `nltk` part-of-speech tagger returns tags in a different format than the lemmitiser expects. We have to do a bunch of faffing around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmy = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    \"\"\"Translate NLTK POS into wordnet POS\"\"\"\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag[0].upper(), wordnet.NOUN)\n",
    "\n",
    "def tokenise(text):\n",
    "    tagged_tokens = nltk.pos_tag(nltk.word_tokenize(text)) # Runs the text through the nltk pos_tagger\n",
    "    return [lemmy.lemmatize(word, get_wordnet_pos(pos_tag)) for word, pos_tag in tagged_tokens] # Passes each word and translated pos tag to lemmatiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['here',\n",
       " 'an',\n",
       " 'example',\n",
       " 'of',\n",
       " 'a',\n",
       " 'sentence',\n",
       " 'that',\n",
       " 'get',\n",
       " 'some',\n",
       " 'benefit',\n",
       " 'from',\n",
       " 'stem',\n",
       " 'lemmatising',\n",
       " 'or',\n",
       " 'it',\n",
       " 'might',\n",
       " 'not']"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenise(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are pretty similar, but \"example\" and \"sentence\" have been spared the chop, since the lemmatiser knows they're nouns and don't get inflected like verbs do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Excercises:__\n",
    "* What are the ten most-used words in burger descriptions, after applying lemmatisation?\n",
    "* Make a `CountVectorizer` that makes character 1-grams of the first letter of each word in the text.\n",
    "* __HARD MODE__: Make a `CountVectorizer` that makes character 2-grams, but _also_ removes English stop-words before making those tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
